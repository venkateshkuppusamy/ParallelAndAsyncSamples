Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously.[1] 
Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of 
parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing,
but it's gaining broader interest due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation)
by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of 
multi-core processors.[4]

Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: 
it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by 
time-sharing on a single-core CPU).[5][6] In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks 
that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often 
do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process 
communication during execution.

Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having
multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer
architectures are sometimes used alongside traditional processors, for accelerating specific tasks.

In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly 
those that use concurrency, are more difficult to write than sequential ones,[7] because concurrency introduces several new classes of potential software bugs, of 
which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting 
optimal parallel program performance.

A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.

Traditionally, computer software has been written for serial computation. To solve a problem, an algorithm is constructed and implemented as a serial stream of 
instructions. These instructions are executed on a central processing unit on one computer. Only one instruction may execute at a time—after that instruction is 
finished, the next one is executed.[8]

Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent
parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources
such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above.[8] Historically parallel computing
was used for scientific computing and the simulation of scientific problems, particularly in the natural and engineering sciences, such as meteorology. This led to the 
design of parallel hardware and software, as well as high performance computing.[9]

Frequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The runtime of a program is equal to the number of 
instructions multiplied by the average time per instruction. Maintaining everything else constant, increasing the clock frequency decreases the average time it takes 
to execute an instruction. An increase in frequency thus decreases runtime for all compute-bound programs.[10] However, power consumption P by a chip is given by the 
equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional to the number of transistors whose inputs change), V is voltage, and 
F is the processor frequency (cycles per second).[11] Increases in frequency increase the amount of power used in a processor. Increasing processor power consumption 
led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer
architecture paradigm.[12]

To deal with the problem of power consumption and overheating the major central processing unit (CPU or processor) manufacturers started to produce power efficient 
processors with multiple cores. The core is the computing unit of the processor and in multi-core processors each core is independent and can access the same memory 
concurrently. Multi-core processors have brought parallel computing to desktop computers. Thus parallelisation of serial programmes has become a mainstream programming 
task. In 2012 quad-core processors became standard for desktop computers, while servers have 10 and 12 core processors. From Moore's law it can be predicted that the 
number of cores per processor will double every 18–24 months. This could mean that after 2020 a typical processor will have dozens or hundreds of cores.[13]

An operating system can ensure that different tasks and user programmes are run in parallel on the available cores. However, for a serial software programme to take 
full advantage of the multi-core architecture the programmer needs to restructure and parallelise the code. A speed-up of application software runtime will no longer 
be achieved through frequency scaling, instead programmers will need to parallelise their software code to take advantage of the increasing computing power of multicore
architectures.[14]